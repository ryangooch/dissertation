\chapter{Discovering the Optimum Model and Increasing Dataset Size}
\label{sec:bestmodel}
% Discuss updates since Prelim, including increasing dataset size and discovery of best model

Various issues arise in the preceding chapters regarding the classification system developed and dataset compiled.
A brief list of such issues includes:

\begin{itemize}
	\item Limited labeled dataset
	\item Restricted generalization ability of the model
	\item Lack of integration of recent developments in image-based machine learning techniques
\end{itemize}

As previously discussed, the initial dataset includes, in total, roughly 1,200 weather radar images.
This dataset was compiled by the author's careful inspection of hundreds of thousands of images and labeled by hand when a particular meteorological phenomena or precipitation regime was present.
The data corpus was constructed of scans from one X-band radar, located in Midlothian, TX, with time coverage between January 2016 to December 2017.
Given that plan-position indicator (PPI) scans are produced at the 1 degree elevation every minute, there are roughly 500,000 scans available each year.
Even with diligent searching and labeling, many potentially valuable radar scans representing phenomena of interest will be missed.

Part of the inspiration for this work, and indeed a major goal, however, was to be able to apply a "less-educated" model to new data, produce what may be referred to as "less-educated guesses" as to class membership on said new data, and allow a human expert the ability to peruse these guesses and correct mistakes made in terms of false positives and false negatives.
This process greatly reduces the time taken to label the many thousands of candidate images and generates an opportunity to increase the dataset size by an order of magnitude.

The second item in the list above goes hand-in-hand with the first item, in that in order to improve the generalization ability of the model, we must be able to train the model using many more representations of classes of interest.
To do this, we must include more data.

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{./thesis_code/plots/DL-workflow.png}
	\caption{Complete end-to-end deep learning architecture devised in this chapter. Note that convolution layers now include batch normalization, while downsampling layers are formed by the new BlurPool layer. These two are tuneable parameters in this architecture, and we tested many configurations to find the best set of parameters and hyperparameters.}
	\label{fig:end-to-end}
\end{figure}

The third item can be used to assist the second item as well. Computer vision, transfer learning, and machine learning in general are currently some of the most active areas of research in science as a whole.
The tools that are being developed, and indeed, that have been developed, can find application in almost every field, and each field in turn can offer insight into improving the tools themselves.
It is essentially impossible for any one researcher or team of researchers to stay abreast of every relevant development related to their field.
However, leveraging insights made from recent discoveries remains useful, important, and essential for any effort in this field.

To that end, we propose a set of experiments to determine the optimal model to use for our transfer learning procedure by incorporating recent discoveries from the field into our deep learning model architecture, and testing each variation on a consistent dataset to determine the best architecture.
The following sections in thhtis chapter detail the efforts made to increase the dataset size, descriptions of the additional model layers, and present the results of the set of experiments that yields the optimum model, given these alterations and improvements.
We illustrate that the optimum model from the tested configurations is shown in the end-to-end architecture diagram in Figure \ref{fig:end-to-end}.

\section{Increased Dataset Size}
\label{sec:bestmodel_dataset}

One of the factors with the largest impact of deep learning model generalization ability is that of training dataset size.
A rule of thumb is that in cases where deep learning is used to solve classification problems regarding several classes, each class needs around 4,000 representative cases in order to provide a comprehensive view of the intrinsic separable features of each class for a deep learning procedure.
Chapter \ref{sec:classifying} identified around 1,200 images in total for all classes combined, the result of a laborious effort of hand-labeling images from the CASA DFW X-band XMDL radar, with scans covering 2016 and 2017.
A major goal of that effort was to develop a process by which the trained model could be applied to unseen data to provide classification estimates.
This process was undertaken in Section \ref{ssec:classifying_discovery}.

We applied the best fit model to data from April 2018 and September 2018.
These months were chosen as it was suspected that each month included data from each class in Stratiform, Convective, and No Precip.
Indeed, this was the case.
Once the predictions were obtained, a trained observer combed through the results and placed incorrect classifications in their correct categories.
The time taken to perform this task was greatly reduced, since the majority of the scans were considered No Precip and thus ignored.
This assumption doubtless led to some false negatives being missed, but there were sufficiently many new images considered and added to the dataset through this process.
Finally, it is worth noting that during this process, the original dataset was hand-corrected for data quality and label-quality issues, resulting in a training dataset more representative of reality than before.

\begin{table}[ht]
	\centering
	\begin{adjustbox}{width=\columnwidth,center}
	\begin{tabular}{c|cccc|cccc|cc}
		& \multicolumn{4}{|c|}{ Convective } & \multicolumn{4}{|c|}{ Stratiform } & \multicolumn{2}{|c}{ No Precipitation } \\
		& Date & No. Scans & Date & No. Scans & Date & No. Scans & Date & No. Scans & Date & No. Scans \\
		\hline
		\multirow{29}{*}{ Training } & 20160509 & 7 & 20180422 & 84 & 20160502 & 57 & 20180903 & 65 & 20161007 & 2 \\ 
		& 20160518 & 32 & 20180425 & 128 & 20160511 & 107 & 20180904 & 28 & 20170607 & 73 \\
		& 20160527 & 31 & 20180426 & 1 & 20160512 & 61 & 20180905 & 108 & 20180401 & 1297 \\
		& 20160705 & 27 & 20180904 & 89 & 20160519 & 64 & 20180906 & 123 & 20180402 & 193 \\
		& 20160725 & 60 & 20180905 & 242 & 20170802 & 60 & 20180907 & 54 & 20180403 & 19 \\
		& 20160727 & 44 & 20180906 & 122 & 20180403 & 15 & 20180908 & 322 & 20180404 & 1 \\
		& 20160728 & 24 & 20180907 & 168 & 20180406 & 23 & 20180909 & 219 & 20180407 & 68 \\
		& 20160729 & 8 & 20180908 & 253 & 20180413 & 154 & 20180911 & 113 & 20180410 & 1 \\
		& 20161007 & 86 & 20180911 & 7 & 20180414 & 58 & 20180912 & 345 & 20180411 & 65 \\
		& 20170102 & 32 & 20180912 & 65 & 20180421 & 172 & 20180913 & 5 & 20180412 & 1 \\
		& 20170307 & 42 & 20180913 & 88 & 20180422 & 5 & 20180914 & 6 & 20180413 & 35 \\
		& 20180403 & 110 & 20180914 & 243 & 20180425 & 301 & 20180915 & 15 & 20180414 & 76 \\
		& 20180406 & 78 & 20180915 & 265 & 20180426 & 48 & 20180922 & 21 & 20180416 & 5 \\
		& 20180407 & 111 & 20180916 & 206 & 20180427 & 67 & & & 20180418 & 109 \\
		& 20180413 & 209 & 20180920 & 135 & & & & & 20180419 & 1 \\
		& 20180414 & 92 & 20180921 & 373 & & & & & 20180421 & 5 \\
		& 20180421 & 174 & 20180922 & 427 & & & & & 20180422 & 11 \\
		& & & & & & & & & 20180423 & 3 \\
		& & & & & & & & & 20180424 & 31 \\
		& & & & & & & & & 20180425 & 127 \\
		& & & & & & & & & 20180428 & 240 \\
		& & & & & & & & & 20180429 & 31 \\
		& & & & & & & & & 20180430 & 57 \\
		& & & & & & & & & 20180921 & 14 \\
		& & & & & & & & & 20180922 & 2 \\
		& & & & & & & & & 20180923 & 1 \\
		& & & & & & & & & 20180924 & 12 \\
		& & & & & & & & & 20180927 & 1 \\
		& & & & & & & & & 20180928 & 3 \\ 
		\hline
		\multirow{22}{*}{ Testing } & 20160530 & 20 & 20180422 & 16 & 20160526 & 92 & 20180427 & 3 & 20170607 & 60 \\ 
		& 20160601 & 10 & 20180425 & 20 & 20160815 & 82 & 20180903 & 532 & 20180401 & 141 \\
		& 20160602 & 16 & 20180903 & 402 & 20180403 & 7 & 20180905 & 7 & 20180402 & 148 \\
		& 20160705 & 25 & 20180905 & 24 & 20180406 & 4 & 20180906 & 11 & 20180403 & 85 \\
		& 20160815 & 18 & 20180906 & 235 & 20180407 & 2 & 20180907 & 5 & 20180404 & 144 \\
		& 20160910 & 21 & 20180907 & 19 & 20180413 & 21 & 20180908 & 37 & 20180405 & 143 \\
		& 20170503 & 60 & 20180908 & 169 & 20180414 & 5 & 20180909 & 38 & 20180406 & 131 \\
		& 20170624 & 31 & 20180912 & 7 & 20180421 & 17 & 20180911 & 12 & 20180407 & 137 \\
		& 20170701 & 21 & 20180913 & 6 & 20180422 & 3 & 20180912 & 44 & 20180408 & 133 \\
		& 20170704 & 30 & 20180914 & 19 & 20180425 & 40 & 20180915 & 2 & 20180409 & 149 \\
		& 20170724 & 26 & 20180915 & 23 & 20180426 & 4 & 20180922 & 238 & 20180411 & 5 \\
		& 20180403 & 12 & 20180916 & 19 & & & & & 20180413 & 5 \\
		& 20180406 & 8 & 20180920 & 10 & & & & & 20180414 & 8 \\
		& 20180407 & 12 & 20180921 & 34 & & & & & 20180416 & 1 \\
		& 20180413 & 20 & 20180922 & 42 & & & & & 20180418 & 13 \\
		& 20180414 & 6 & 20180926 & 22 & & & & & 20180421 & 1 \\
		& 20180421 & 16 & 20180929 & 137 & & & & & 20180422 & 1 \\
		& & & & & & & & & 20180424 & 1 \\
		& & & & & & & & & 20180425 & 18 \\
		& & & & & & & & & 20180428 & 17 \\
		& & & & & & & & & 20180429 & 8 \\
		& & & & & & & & & 20180430 & 6 \\ 
	\end{tabular} 
	\end{adjustbox}
	\caption{Dates and numbers of scans for each training and testing datasets. All scans recorded at the XMDL X-band radar in Midlothian, TX. Note, the larger numbers of scans in September 2018 correspond with the first month of data that our model was deployed upon to find scans of interest, with false classifications being placed according to their true label where necessary.}
	\label{table:bestmodel_data_details}
\end{table}

As a result of this process, many scans were added to the dataset and allowed some of the overfitting issues seen in the prior sections to be corrected, while gaining classification accuracy.
The dataset size increase by an order of magnitude, resulting in a total of around 12,000 images being hand-labeled into categories of Stratiform, Convective, and No Precip.
The dates and counts per date for each scan are detailed in Table \ref{table:bestmodel_data_details} for the sake of future efforts that may require knowledge of particular precipitation regimes in this geographic area and time frame.
It is clear that even the first pass model that was limited by smaller amounts of independent training data was able to increase the dataset size efficiently and profoundly.
The majority of labeled scans now encompass April 201 and September 2018.

In the following experiments, this dataset is that which will be utilized.



\section{BlurPool}
\label{sec:bestmodel_blurpool}

A dataset must include many representations of the desired phenomena of interest in order to allow better generalization on test sets.
In the specific case of weather radar data, however, it is likely to see scans that are highly visually similar to one another when successive scans are recorded in one minute intervals, as the images in this study are produced.
This high level of inter-image intercorrelation may lead to instances of overfitting, if the training dataset is not large enough to present many uncorrelated events, as well as many scans for each event.
Storms and weather precipitation events will tend to move slowly across the high-resolution, large geographic sampling area present in each scan, leading to the same or similar echoes occurring in tens of successive scans as represented in Figure \ref{fig:bestmodel_translation}.
It is thus desirable to not only encourage generalizability to unseen data in a particular classifier, but in this case, to also be resilient to image object translation.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./thesis_code/plots/midlothian.tx-20180421-235038-ref.png}
		\includegraphics[width=\textwidth]{./thesis_code/plots/dfw_colormap.png}
		\caption{2018-04-21 23:50:38 UTC}
		\label{fig:bestmodel_translation1}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./thesis_code/plots/midlothian.tx-20180421-235141-ref.png}
		\includegraphics[width=\textwidth]{./thesis_code/plots/dfw_colormap.png}
		\caption{2018-04-21 23:51:41 UTC}
		\label{fig:bestmodel_translation2}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./thesis_code/plots/midlothian.tx-20180421-235238-ref.png}
		\includegraphics[width=\textwidth]{./thesis_code/plots/dfw_colormap.png}
		\caption{2018-04-21 23:52:38 UTC}
		\label{fig:bestmodel_translation3}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./thesis_code/plots/midlothian.tx-20180421-235338-ref.png}
		\includegraphics[width=\textwidth]{./thesis_code/plots/dfw_colormap.png}
		\caption{2018-04-21 23:53:38 UTC}
		\label{fig:bestmodel_translation4}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./thesis_code/plots/midlothian.tx-20180421-235438-ref.png}
		\includegraphics[width=\textwidth]{./thesis_code/plots/dfw_colormap.png}
		\caption{2018-04-21 23:54:38 UTC}
		\label{fig:bestmodel_translation5}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./thesis_code/plots/midlothian.tx-20180421-235538-ref.png}
		\includegraphics[width=\textwidth]{./thesis_code/plots/dfw_colormap.png}
		\caption{2018-04-21 23:55:38 UTC}
		\label{fig:bestmodel_translation6}
	\end{subfigure}
	\caption{A particular precipitation event as observed in successive scans. Note the similarity between successive scans. In an ideal classifier, all would be classified as "Convective."}
	\label{fig:bestmodel_translation}
\end{figure}

Convolutional neural networks like VGG16 are designed to be resilient to this type of translation, but a recent paper \cite{zhang2019making} illustrated that by replacing the downsampling layer which follows a convolution block, we can gain both accuracy and consistency of prediction.
This layer is called BlurPool, as it is a pooling layer that utilizes a Gaussian blur filter kernel to first low-pass filter the image feature representations following a particular convolution block, before applying the downsampling mechanism.

An important step in any deep learning classifier is the downsampling layer, which takes as input a wider, shallower set of image representations, and reduces the width while increasing the depth.
Width in this context refers to what can be thought of as the number of pixels in the image or image representation, and depth, the number of channels or activations.
We wish to take a high-dimensional input (image), and produce a low dimensional set of predictions (classes).
Thus, these downsampling layers are of critical importance to the deep learning process.

The authors of this paper argue that the current set of most commonly used downsampling layers, however, is not as resilient to translation in input image objects as would be desirable.
They draw inspiration from signal processing, whereby a signal must be low-pass filtered prior to down-sampling, to mitigate issues arising from potential high frequency energy aliasing into lower frequencies.
The current most often used downsampling layers in deep learning models, like max pooling or average pooling, essentially allow this sort of aliasing.
With a Gaussian filter kernel applied to activation layers prior to  downsampling, we can mitigate or eliminate this type of contamination, effectively anti-aliasing the deep learning operations and promoting shift-invariance in the model.

\section{BatchNormalization}
\label{sec:bestmodel_batchnorm}

We have presented some treatment of the Batch Normalization process elsewhere, though this section aims to more deeply describe its place in deep learning model research, its added value for this effort, and some implementation strategies used to adapt the off-the-shelf VGG16 architecture to include Batch Normalization in its convolution blocks.

As described above, the batch normalization, or BatchNorm or BN, layer seeks to normalize and standardize the activations in deep neural networks in order to minimize the effects of covariance shift and reduce issues arising from vanishing and exploding gradients.
This effect is confirmed in \cite{bjorck2018understanding}, who illustrates several interesting features of BN in deep neural networks. 
One insight gained was in examining activation weight ratios in intermediate layers in DNNs trained both with and without BN. 
Those trained with BN had much larger ratios and more variable values in gradients than did the DNN trained without BN.
This indicates that without BN, activations tend to converge to the same values regardless of the input data, indicating a covariate shift.
With BN, however, the data more deeply influences intermediate layers.
Additionally, it was shown that empirical distributions of gradient values with BN exhibited a distribution more reminiscent of a Gaussian curve, lower kurtosis, and slimmer tails, which shows that the gradient values were more clustered around the mean value of zero.
Furthermore, it was demonstrated that gradient update step size created less divergence in the relative loss functions across many mini-batch sizes.
This result validates our usage of a variable learning parameter mechanism in part, since the loss is not adversely affected for decreasing step size.
Finally, it was shown that weights in intermediate layers tended to converge to low-rank, more variable values in the DNN without BN, whereas the DNN trained with BN had a less marked effect in these terms.

Theoretically, it seems clear that batch normalization layers can and will help in our experiment as well, but there is also a large corpus of practical evidence to support this hypothesis as well.
The fundamental VGG16 architecture \cite{simonyan2014very} that we have built our base architecture upon was chosen for its demonstrated success in transferring learning effectively to target tasks that were much different than source tasks, but as it was introduced in 2014, the model does not include any BN layers, since they had not yet been discovered.
The paper also introduced a variation called VGG19, which included more layers and represented a larger network, but again, it did not include BN.

The Extreme Inception, or Xception, DNN includes batch normalization in all convolution and separable convlution layers \cite{chollet2017xception}.
It was based in part on a style of network called \textit{Inception}, of which there exist multiple versions.
The third major version of this network, InceptionV3, batch normalizes all output activations \cite{szegedy2016rethinking}.
Another iteration of this style of network involves its combination with the ResNet style of architecture, and the combination, referred to as Inception-ResNet, includes some BN layers \cite{szegedy2017inception}.
This is of particular interest, since the authors implemented their network using Tensorflow \cite{tensorflow2015-whitepaper}, and discovered that implementing BN for every activation layer led to greatly increased memory usage and slower speed.
Our networks also use Tensorflow, and our results indicate a similar effect, where networks with BN spent more time in training and necessitated a smaller mini-batch size.
However, BN was still utilized where it was deemed reasonable and necessary to do so.

One final class of deep neural network that is worth mentioning is the MobileNet \cite{howard2017mobilenets}.
These networks take their name from their intended use space; mobile devices.
On such devices, trained networks must have a lower data footprint.
This makes these networks applicable to situations and modalities where compute-limited or otherwise costrained compute systems must be used, as in Internet of Things sensor setups.
This comes at a cost of accuracy and precision in learning classifiable deep feature representations, and in preliminary testing, our VGG16 convolutional base outperformed MobileNet feature extractors.
Still, it is important to note that in MobileNets, BN is present.

We thus consider batch normalization to be a necessary layer to add to our architecture, and we will illustrate an experiment below that compares networks with and without BN and demonstrates the layer's efficacy in improving classifications.

\section{Experiment Setup}
\label{sec:bestmodel_experiment}

The diagram in Figure \ref{fig:end-to-end} demonstrates the architecture configuration we use in this set of experiments.
We included both BlurPool and batch normalization layers in the diagram to illustrate where these layers would exist in certain configurations.

The goal of this experiment is to test the end-to-end deep learning model to discover the optimal set of parameters and hyperparameters for the expanded weather radar image dataset.
There are a few differences from the earlier experiments presented in this work:

\begin{itemize}
	\item Increased dataset;
	\item Usage of batch normalization layers;
	\item Downsampling via BlurPool layers, and;
	\item Implementation strategy.
\end{itemize}

We have detailed above the efforts to increase the dataset using data from 2018 and performing first stage classifications with the deep neural network presented in Chapter \ref{sec:classifying}.
This experiment will seek to test the effectiveness of using BN layers after each activation to manage gradients and reduce covariate shift, while also comparing the performance of low-pass filtered, shift-invariant downsampling operations of BlurPool with the default average or max pooling operations predominant in most off-the-shelf deep neural networks.
Finally, this approach involves a deeper implementation strategy in Tensorflow, whereby the BlurPool activation was developed from scratch, and the BN layers were added via custom inclusion to all convolutional layers in the feature extractor portion of the end-to-end model architecture.

Specifically, we tested nine configurations, as shown in Table \ref{table:bestmodel_experiment_results}.
As can be seen, the BlurPool parameter of \textit{kernel size} governs the filter kernel that is used to perform the downsampling operation.
A kernel size of 1 produces a 1x1 convolution and thus, offers no desirable filtering characteristics.
The kernel size of 2 yields a 2x2 filter kernel that is essentially an average pooling operation, which gives a useful comparison to an industry-standard downsampling mechanism.
The kernel BlurPool (k=2) is given by:

\[
k=\frac{1}{N}
	\begin{bmatrix}
	1 & 1 \\
	1 & 1
	\end{bmatrix}
\]

where $N$ is given by the sum of all elements in the matrix. Here, $N=4$.

The kernel BlurPool (k=3) is given by:

\[
k=\frac{1}{N}
	\begin{bmatrix}
	1 & 2 & 1 \\
	2 & 4 & 2 \\
	1 & 2 & 1	
	\end{bmatrix}
\]

The kernel BlurPool (k=5) is given by:

\[
k= \frac{1}{N}
	\begin{bmatrix}
	 1 & 4 & 6 & 4 & 1 \\
	 4 & 16 & 24 & 16 & 4 \\
	 6 & 24 & 36 & 24 & 6 \\
	 4 & 16 & 24 & 16 & 4 \\
	 1 & 4 & 6 & 4 & 1
	\end{bmatrix}
\]

The kernel BlurPool (k=7) is given by:

\[
k=\frac{1}{N}
	\begin{bmatrix}
	  1 & 6 & 15 & 20 & 15 & 6 & 1 \\
	  6 & 36 & 90 & 120 & 90 & 36 & 6 \\
	 15 & 90 & 225 & 300 & 225 & 90 & 15 \\
	 20 & 120 & 300 & 400 & 300 & 120 & 20 \\
	 15 & 90 & 225 & 300 & 225 & 90 & 15 \\ 
	  6 & 36 & 90 & 120 & 90 & 36 & 6 \\
	  1 & 6 & 15 & 20 & 15 & 6 & 1
	\end{bmatrix}
\]
% table showing results of all tests
%\begin{table*}[t]
%	\centering
%	\begin{tabular}{ccccc}
%		& & \multicolumn{3}{c}{ Three Class Averaged Classification Statistics } \\
%		Configuration Details & Name & Categorical Accuracy & Precision & Recall \\
%		\hline
%		Default VGG16 Architecture & VGG16 & 0.9208 & 0.9249 & \textbf{0.9165} \\
%		VGG16 with default convolution and BlurPool downsampling (kernel size of 2) & DC\_BP (k=2) & 0.8958 & 0.9071 & 0.8830 \\
%		VGG16 with default convolution and BlurPool downsampling (kernel size of 3) & DC\_BP (k=3) & 0.8807 & 0.8901 & 0.8720 \\
%		VGG16 with default convolution and BlurPool downsampling (kernel size of 5) & DC\_BP (k=5) & 0.9036 & 0.9143 & 0.8916 \\
%		VGG16 with default convolution and BlurPool downsampling (kernel size of 7) & DC\_BP (k=7) & 0.8946 & 0.9053 & 0.8824 \\
%		VGG16 with convolution + batch normalization and BlurPool downsampling (kernel size of 2) & BN\_BP (k=2) & 0.9160 & 0.9238 & 0.9148 \\
%		VGG16 with convolution + batch normalization and BlurPool downsampling (kernel size of 3) & BN\_BP (k=3) & 0.9160 & 0.9238 & 0.9148 \\
%		VGG16 with convolution + batch normalization and BlurPool downsampling (kernel size of 5) & BN\_BP (k=5) & \textbf{0.9220} & \textbf{0.9329} & 0.9161 \\
%		VGG16 with convolution + batch normalization and BlurPool downsampling (kernel size of 7) & BN\_BP (k=7) & 0.9198 & 0.9305 & 0.9160 \\
%		\hfill
%	\end{tabular} 
%	\caption{Comprehensive results of each deep learning configuration tested using the full dataset. The values reported reflect the class-weighted average for both Precision and Recall. The Name column matches annotations on Figure \ref{fig:precision_recall_curves}. Bolding accentuates best result in each column.}
%	\label{table:experiment_results}
%\end{table*}

Model predictions were primarily evaluated on three criteria that each offer insight into classification ability.
Specifically, we computed the three-class averaged results for categorical accuracy, precision, and recall.
Categorical accuracy consists of the ratio of all true positive results to the sum of all datapoints, and is given by

\begin{equation}
\mathrm{Categorical Accuracy} = \frac{1}{N}\sum_{c=1}^{C}TP_c
\end{equation}

where TP stands for True Positive and refers to a correct classification, $N$ is the total number of images classified, and $C$ is the number of classes.

Precision is given by

\begin{equation}
\mathrm{Precision} = \frac{TP}{TP+FP}
\end{equation}

where $FP$ is the number of False Positives with respect to a class of interest.
This metric details the percentage of classifications in a category that are the class of interest.

Finally, recall is given by

\begin{equation}
\mathrm{Recall} = \frac{TP}{TP+FN}
\end{equation}

where $FN$ is the number of False Negatives with respect to a class of interest.


\section{Results}
\label{sec:bestmodel_results}

\begin{table}[ht]
	\centering
	\begin{tabular}{ccccc}
		& & \multicolumn{3}{c}{ Three Class Avg Classification Stats } \\
		Configuration Details & Name & Categorical & Precision & Recall \\
		&      & Accuracy    &           &        \\
		Default VGG16 Architecture & VGG16 & 0.9208 & 0.9249 & \textbf{0.9165} \\
		& & & & \\
		VGG16 with default convolution and & DC\_BP (k=2) & 0.8958 & 0.9071 & 0.8830 \\
		BlurPool downsampling (k=2) & & & & \\
		VGG16 with default convolution and & DC\_BP (k=3) & 0.8807 & 0.8901 & 0.8720 \\
		BlurPool downsampling (k=3) & & & & \\
		VGG16 with default convolution and & DC\_BP (k=5) & 0.9036 & 0.9143 & 0.8916 \\
		BlurPool downsampling (k=5) & & & & \\
		VGG16 with default convolution and & DC\_BP (k=7) & 0.8946 & 0.9053 & 0.8824 \\
		BlurPool downsampling (k=7) & & & & \\
		VGG16 with convolution + BatchNorm & BN\_BP (k=2) & 0.9160 & 0.9238 & 0.9148 \\
		and BlurPool downsampling (k=2) & & & & \\
		VGG16 with convolution + BatchNorm & BN\_BP (k=3) & 0.9160 & 0.9238 & 0.9148 \\
		and BlurPool downsampling (k=3) & & & & \\
		VGG16 with convolution + BatchNorm & BN\_BP (k=5) & \textbf{0.9220} & \textbf{0.9329} & 0.9161 \\
		and BlurPool downsampling (k=5) & & & & \\
		VGG16 with convolution + Batch Norm & BN\_BP (k=7) & 0.9198 & 0.9305 & 0.9160 \\
		and BlurPool downsampling (k=7) & & & & \\
	\end{tabular}
	\caption{Comprehensive results of each deep learning configuration tested using the full dataset. The values reported reflect the class-weighted average for both Precision and Recall. The Name column matches annotations on Figure \ref{fig:bestmodel_precision_recall_curves}. Bolding accentuates best result in each column.}
	\label{table:experiment_results}
\end{table}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.85\textwidth]{./thesis_code/plots/precision_recall_curve_all_configs_bw.png}
	\caption{Precision and Recall statistics for each configuration tested. See Table \ref{table:experiment_results} for configurations corresponding to the names in this figure.}
	\label{fig:bestmodel_precision_recall_curves}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{./thesis_code/plots/20191011_BP_k5_BN_loss_bw.png}
		\caption{Loss}
		\label{fig:bestmodel_first_case}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{./thesis_code/plots/20191011_BP_k5_BN_acc_bw.png}
		\caption{Accuracy}
		\label{fig:bestmodel_second_case}
	\end{subfigure}
	\caption{Loss and accuracy curves for our best-case network, which utilized the VGG16 base model feature extractor, with batch normalization layers following each convolutional layer, and BlurPool layers with a kernel size of 5. Training was allowed to progress with base model weights frozen until a loss plateau was reached, before fine-tuning by allowing all parameters to update during training. Fine Tuning began at Epoch 36.}
	\label{fig:bestmodel_fig_sim}
\end{figure}

The results of the described experiments can be seen in Table \ref{table:experiment_results}.
Interestingly, the default configuration given by VGG16 yields very good results, and indeed, has the highest Recall value of any configuration tested.
However, the best overall architecture was that which utilized both batch normalization and BlurPool (k=5).
It registered the highest categorical accuracy as well as the highest precision, and its recall was nearly equivalent to the best-in-class default VGG16.

\section{Classifications on Unseen Data}
\label{sec:bestmodel_newdata}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./thesis_code/plots/2019_predictions_hist_bw.png}
	\caption{The fully trained model was deployed on all scans available from 2019 observed and recorded by the CASA XMDL radar. This set of plots shows the statistics on how images were classified for all months together (top left), and each month individually.}
	\label{fig:bestmodel_all_hists}
\end{figure}

\section{Summary}
\label{sec:bestmodel_summary}

As seen above, the added data has led to a greater level of generalization ability for the deep neural network architecture.
The addition of BlurPool appears to help not only the consistency and generalizability of the model, but also the classification accuracy.
Batch Normalization ensures the training parameters are well-behaved and thus converge to a better minimum in the loss function.
And the newly trained model quickly and efficiently produces believable classifications on large amounts of data in a short period of time.


